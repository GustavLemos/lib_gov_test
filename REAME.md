
---

# gov_credit

`gov_credit` Ã© uma biblioteca de **governanÃ§a e qualidade de dados** para ambientes Spark, projetada para funcionar **com a mesma base de cÃ³digo** em:

* **Databricks** (Metastore / Unity Catalog)
* **Google Colab / Spark local** (Delta Lake por path)

A biblioteca registra automaticamente **datasets**, **colunas** e aplica **regras de qualidade** carregadas dinamicamente em tempo de execuÃ§Ã£o.

---

## ğŸ¯ Objetivos da biblioteca

* GovernanÃ§a automÃ¡tica sem acoplamento ao pipeline
* Registro incremental de metadados (datasets e colunas)
* Regras externas, versionÃ¡veis e dinÃ¢micas
* CompatÃ­vel com Databricks e Colab
* Baseada em **Delta Lake**
* Arquitetura extensÃ­vel (Data Mesh / Lakehouse)

---

## ğŸ“ Estrutura do projeto

```
gov_credit/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ engine.py
â”‚
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ databricks.py
â”‚   â””â”€â”€ colab.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ databricks_create_dq_tables.sql
â”‚   â””â”€â”€ colab_create_dq_tables.py
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ databricks_example.py
â”‚   â””â”€â”€ colab_example.py
â”‚
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ§  Arquitetura (conceito)

A biblioteca Ã© dividida em trÃªs camadas:

1. **Core (agnÃ³stico de ambiente)**
2. **Adapters (implementaÃ§Ã£o por ambiente)**
3. **Camada de metadados (tabelas DQ)**

O **core nunca sabe onde estÃ¡ rodando**.
Quem decide como ler e escrever metadados Ã© o **adapter**.

---

## ğŸ“‚ `core/engine.py`

### Responsabilidade

Orquestrar o fluxo de governanÃ§a e qualidade de dados.

### O que ele faz

1. Garante que o dataset esteja registrado
2. Garante que colunas novas sejam registradas
3. Carrega regras ativas
4. Aplica regras (ou ignora se nÃ£o existirem)

### FunÃ§Ãµes principais

#### `DataQualityEngine.__init__(adapter)`

* Recebe um adapter (Databricks ou Colab)
* Injeta o Spark correto
* Desacopla a lÃ³gica do ambiente

#### `DataQualityEngine.run(df, dataset_name)`

* Ponto Ãºnico de entrada da lib
* Deve ser chamado **antes do write**
* Nunca quebra pipeline por falta de regra

---

## ğŸ“‚ `adapters/base.py`

### Responsabilidade

Definir o **contrato mÃ­nimo** que qualquer ambiente precisa implementar.

### Por que existe

* Evita `if colab / if databricks`
* Permite novos ambientes no futuro (EMR, Glue, Synapse)

### MÃ©todos obrigatÃ³rios

* `ensure_dataset`
* `ensure_columns`
* `load_rules`

---

## ğŸ“‚ `adapters/databricks.py`

### Responsabilidade

Implementar governanÃ§a usando:

* Metastore
* Tabelas gerenciadas
* SQL nativo do Databricks

### Comportamento

* Usa `INSERT ... WHERE NOT EXISTS`
* Nunca duplica registros
* Trabalha com `dq.datasets`, `dq.columns`, `dq.rules`

### Quando usar

* ProduÃ§Ã£o
* Ambientes corporativos
* Unity Catalog

---

## ğŸ“‚ `adapters/colab.py`

### Responsabilidade

Implementar governanÃ§a usando:

* Delta Lake por **path**
* Spark local
* Sem metastore

### Comportamento

* Cria tabelas Delta se nÃ£o existirem
* Registra apenas colunas novas
* Funciona com S3, GCS, ADLS ou filesystem local

### Quando usar

* Provas de conceito
* Estudos
* LaboratÃ³rio
* Desenvolvimento local

---

## ğŸ“‚ `sql/databricks_create_dq_tables.sql`

### Por que este script Ã© necessÃ¡rio

A biblioteca **nÃ£o cria tabelas automaticamente**.

Isso Ã© proposital.

Criar estruturas de governanÃ§a Ã©:

* decisÃ£o de plataforma
* decisÃ£o de seguranÃ§a
* decisÃ£o organizacional

### O que ele cria

* `dq.datasets` â†’ catÃ¡logo de datasets
* `dq.columns` â†’ catÃ¡logo de colunas
* `dq.rules` â†’ regras de qualidade

### Quando executar

* Uma Ãºnica vez por workspace
* Ou via pipeline de bootstrap

---

## ğŸ“‚ `sql/colab_create_dq_tables.py`

### Por que existe

No Colab:

* NÃ£o existe metastore
* NÃ£o existe SQL DDL persistente

Este script cria **os mesmos contratos**, porÃ©m:

* via DataFrame vazio
* usando Delta Lake por path

### Resultado

```
/content/delta/dq/
â”œâ”€â”€ datasets/
â”œâ”€â”€ columns/
â””â”€â”€ rules/
```

Esses paths sÃ£o consumidos diretamente pelo `ColabAdapter`.

---

## ğŸ“‚ `examples/`

### Objetivo

Mostrar **uso real**, nÃ£o toy examples.

#### `databricks_example.py`

* Usa Spark do Databricks
* Usa `DatabricksAdapter`

#### `colab_example.py`

* Cria Spark local
* Usa `ColabAdapter`
* LÃª e escreve Delta por path

---

## ğŸ“¦ `setup.py`

### Responsabilidade

Permitir:

* instalaÃ§Ã£o via Git
* uso em Databricks (`%pip install`)
* uso em Colab (`pip install`)

---

## ğŸ” Fluxo de execuÃ§Ã£o resumido

1. Executa script SQL de bootstrap
2. Pipeline cria DataFrame
3. Chama `engine.run(df, dataset)`
4. GovernanÃ§a acontece automaticamente
5. Pipeline segue normalmente

---

## ğŸ§  DecisÃµes arquiteturais importantes

* âŒ A lib nÃ£o cria tabelas

* âŒ A lib nÃ£o impÃµe regras

* âŒ A lib nÃ£o quebra pipelines sem regra

* âœ… Contrato fixo

* âœ… Regras externas

* âœ… EvoluÃ§Ã£o segura

---
